{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_HW1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liranbd1/Fashion_MNIST_FFNN_HW/blob/main/Deep_Learning_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3btyarBXZt8"
      },
      "source": [
        "**NOTES ON WHAT TO WORK NEXT**\n",
        "\n",
        "1. Connect this notebook to my Google Drive\n",
        "2. Change the save methods to direct data into my Google Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h72yyOG58Hn"
      },
      "source": [
        "Importing external libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWsOTh6942vY"
      },
      "source": [
        "import numpy as np\n",
        "import torch as torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from google.colab import files\n",
        "import inspect\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J55jE0b5AwBs"
      },
      "source": [
        "**Generic Feed-Forward Network**\n",
        "\n",
        "* n_features - number of input features for the network\n",
        "\n",
        "* n_hidden_units_per_layer - Expecting a list of integers where the index  represent the layer and value the number of neurons.\n",
        "\n",
        "* n_outputs - Number of neurons in the output layer\n",
        "\n",
        "* activation_fun - The activation function for all the levels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tybfKplcIHm9"
      },
      "source": [
        "class GenericFeedForwardNetwork(torch.nn.Module):\n",
        "  def __init__(self, n_features, n_hidden_units_per_layer, n_outputs, activation_fun):\n",
        "    non_linear_act_fun = {'relu': torch.nn.ReLU, 'tanh': torch.nn.Tanh, 'sigmoid': torch.nn.Sigmoid, 'Non': None}\n",
        "    super().__init__()\n",
        "    dim_list = [n_features, *n_hidden_units_per_layer, n_outputs]\n",
        "    layers = []\n",
        "     \n",
        "    for index, (in_dim, out_dim) in enumerate(zip(dim_list[:-1], dim_list[1:])):\n",
        "      if non_linear_act_fun[activation_fun] !=None and index != len(dim_list) - 1:\n",
        "        layers += [\n",
        "                  torch.nn.Linear(in_dim, out_dim, bias = True),\n",
        "                  non_linear_act_fun[activation_fun]()  \n",
        "                  ]\n",
        "      else:\n",
        "        layers += [\n",
        "                  torch.nn.Linear(in_dim, out_dim, bias = True)\n",
        "                  ]\n",
        "\n",
        "    self.fc_layers = torch.nn.Sequential(*layers)\n",
        "    self.softmax = torch.nn.LogSoftmax(dim = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = self.fc_layers(x)\n",
        "    y_pred = self.softmax(h)\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c9Yl6ziBxQ1"
      },
      "source": [
        "**Utility Functions**\n",
        "\n",
        "training - Refactoring the training loop for more modularity \n",
        "* epochs - number of times we want to go over the whole dataset\n",
        "\n",
        "* learning_rate - The learning rate \n",
        "\n",
        "* optimizer - The optimizer function \n",
        "\n",
        "* loss_function - The loss function \n",
        "\n",
        "* model - A Feed-Forward neural network model\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "calculate_accuracy - Calculate the accuracy of a given model \n",
        "* data_loader - The DataLoader object we want to use as the test set for the model\n",
        "* model - A Feed-Forward neural network model\n",
        "\n",
        "---\n",
        "plot_train_validation_by_epochs - Reciving the data of the epochs, train and validation accuracy to plot two line charts, we refactored this code to a utility function since it is asked in every function\n",
        "* epochs - The number of epochs to create our x-axis\n",
        "* train_acc - a list of all the training accuracy for each epochs\n",
        "* validation_acc - a list of all the validation accuracy for each epochs\n",
        "\n",
        "All the indices are synced meaning that the first index in train_acc and validation_acc are the first accuarcy values for the 1st epoch\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48qzCZ-KY-Gt"
      },
      "source": [
        "def training(epochs, optimizer, loss_function, model, scheduler = None, train_DL = None, calc_loss=False, patience = -1):\n",
        "  train_acc = []\n",
        "  val_acc = []\n",
        "  loss_list_train = []\n",
        "  loss_list_validation = []\n",
        "  stopping_epoch = epochs\n",
        "  min_epoch = 0\n",
        "  minimal_loss = math.inf\n",
        "  patience_count = 1000 #Random higg value\n",
        "  train_dataloader = train_data if train_DL == None else train_DL\n",
        "  if patience > 0:\n",
        "    patience_count = patience \n",
        "  for i in range(epochs):\n",
        "    current_epoch = i \n",
        "    for j, (data, label) in enumerate(train_dataloader):\n",
        "      optimizer.zero_grad()\n",
        "      data = data.view(-1, input_size).to(device)\n",
        "      y_hat = model(data)\n",
        "      loss = loss_function(y_hat, label.to(device))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "   \n",
        "    if scheduler != None:\n",
        "      scheduler.step()\n",
        "    if calc_loss: # If we are calculating loss per epochs\n",
        "      loss_list_train.append(loss.detach()) # Saving the Training loss\n",
        "      model.eval() # Letting the model now we are evaluating so no updates done\n",
        "      with torch.no_grad(): # Saving on memory by not calculating the grads during eval\n",
        "        # Simulate train loop on validation data\n",
        "        for j, (data, label) in enumerate(validation_data):\n",
        "          optimizer.zero_grad()\n",
        "          data = data.view(-1, input_size).to(device)\n",
        "          y_hat = model(data)\n",
        "          loss = loss_function(y_hat, label.to(device))\n",
        "\n",
        "          optimizer.step()\n",
        "      \n",
        "        if scheduler != None:\n",
        "          scheduler.step()\n",
        "        loss_value = loss.detach()  \n",
        "        loss_list_validation.append(loss_value) #Saving Validation loss\n",
        "        if loss_value < minimal_loss:\n",
        "          minimal_loss = loss_value\n",
        "          torch.save(model.state_dict(), \"./data/best_model.pth\")\n",
        "          min_epoch = i\n",
        "          patience_count = patience\n",
        "        elif loss_value > minimal_loss:\n",
        "          patience_count -= 1\n",
        "       \n",
        "      model.train() # indicating we are going back to train\n",
        "    \n",
        "    train_acc.append(calculate_accuracy(train_dataloader, model))\n",
        "    val_acc.append(calculate_accuracy(validation_data, model))\n",
        "    if patience_count == 0:\n",
        "      stopping_epoch = current_epoch+1 # Since we are starting from 0\n",
        "      break\n",
        "  \n",
        "  if calc_loss:\n",
        "    return train_acc, val_acc, loss_list_train, loss_list_validation, stopping_epoch, min_epoch\n",
        "  else:\n",
        "    return train_acc, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YIuaqzhalzb"
      },
      "source": [
        "def calculate_accuracy(data_loader, model, line_title = None):\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for j, (data,label) in enumerate(data_loader):\n",
        "    data = data.view(-1, input_size).to(device)\n",
        "    y_hat = model(data)\n",
        "    predictions = torch.argmax(y_hat, dim=1)\n",
        "    correct_count += torch.sum(predictions == label.to(device)).type(torch.float32)\n",
        "    total_count += data.shape[0]\n",
        "  accuracy = (correct_count/ total_count).item()*100\n",
        "  \n",
        "  # Real world scenarion I would save the accuracies to a dictionary \n",
        "  # and then write the dict into a Json file.\n",
        "  if line_title != None:\n",
        "    with open(f\"./data/accuracy_files/{line_title}.txt\", 'w') as file:\n",
        "      file.write(f\"accuracy : {accuracy} %\")\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsQCr9cPS1vg"
      },
      "source": [
        "def plot_report(x_axis, title, y_axis, number_of_neurons):\n",
        "  if type(x_axis) == int:\n",
        "    x_axis_list = list(range(x_axis))\n",
        "  else:\n",
        "    x_axis_list = x_axis\n",
        "  file_name = f\"{inspect.stack()[1].function}_{number_of_neurons}_{title}\"\n",
        "  file_path = f\"./data/plots/{file_name}\"\n",
        "  fig = plt.figure()\n",
        "  plt.plot(x_axis_list, y_axis, 'r')\n",
        "  plt.title(f\"{title}\")\n",
        "\n",
        "  fig.savefig(file_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-KqdE8QDLd2"
      },
      "source": [
        "**Question 1**\n",
        "\n",
        "Loading the datasets, with the transform showed in the Tirgol.\n",
        "\n",
        "After loading the train dataset we split it to 80/20 by randominzing the indices and using the sampler attribute of the DataLoader.\n",
        "\n",
        "All three DataLoaders are returned from the function to global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw394DbK6lN2"
      },
      "source": [
        "def load_dataset():\n",
        "  # Transform data\n",
        "  normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "  totensor = transforms.ToTensor()\n",
        "  fashion_mnist_transform = transforms.Compose([totensor, normalize])\n",
        "  # Loading the train and test datasets \n",
        "  init_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform= fashion_mnist_transform)\n",
        "  test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform= fashion_mnist_transform)\n",
        "  train_size = int(len(init_dataset)*0.8)\n",
        "  validation_size = int(len(init_dataset)*0.2) \n",
        "  # Splitting the train dataset to train and validation sets      \n",
        "  train_set, validation_set = torch.utils.data.random_split(init_dataset, [train_size, validation_size])\n",
        "  \n",
        "  # Creating DataLoaders\n",
        "  trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size = 64)\n",
        "  validationDataLoader = torch.utils.data.DataLoader(validation_set, batch_size = 64)\n",
        "  testDataLoader = torch.utils.data.DataLoader(test_set,64, shuffle=False)\n",
        " \n",
        "  return trainDataLoader, validationDataLoader, testDataLoader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttsUC_m6FlCx"
      },
      "source": [
        "**Global Variabels**\n",
        "\n",
        "The global variabels are set after all the basic and utilities functions are defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKfu1CxR1E2"
      },
      "source": [
        "# Hyper Parametesr\n",
        "\n",
        "input_size = 28*28 #size of each image\n",
        "output_size = 10\n",
        "train_data, validation_data, test_data = load_dataset()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # One liner to set device to use GPU if avaiable else to use CPU\n",
        "accuracy_folder_path = \"./data/accuracy_files\"\n",
        "plots_folder_path = \"./data/plots\"\n",
        "\n",
        "if not os.path.exists(accuracy_folder_path):\n",
        "  os.makedirs(accuracy_folder_path)\n",
        "  \n",
        "if not os.path.exists(plots_folder_path):\n",
        "  os.makedirs(plots_folder_path)\n",
        "\n",
        "print(f\"train_data: {len(train_data.dataset)}\")\n",
        "print(f\"val_data: {len(validation_data.dataset)}\")\n",
        "print(f\"test_data: {len(test_data.dataset)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn5PU1RGFwpE"
      },
      "source": [
        "**Function 2**\n",
        "\n",
        "One hidden layer no activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8o59WNaYTEn"
      },
      "source": [
        "# Training the model Question 1\n",
        "\n",
        "def one_hidden_layer_no_activation(number_of_neurons):\n",
        "  epochs = 50\n",
        "  learning_rate = 0.01\n",
        "  model = GenericFeedForwardNetwork(input_size, [number_of_neurons], output_size, \"Non\").to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
        "  los_fun = torch.nn.CrossEntropyLoss()\n",
        "  # Take training loop out to a different function \n",
        "  train_acc, validation_acc = training(epochs, optimizer, los_fun, model)\n",
        "  plot_report(epochs, \"train accuracy by epochs\", train_acc, number_of_neurons)\n",
        "  plot_report(epochs, \"validation accuracy by epochs\", validation_acc, number_of_neurons)\n",
        "  \n",
        "  test_acc= calculate_accuracy(test_data, model, f\"{inspect.stack()[0].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-PyoekfWtY6"
      },
      "source": [
        "**Function 3**\n",
        "\n",
        "Two hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wtFUUt9WshD"
      },
      "source": [
        "def two_hidden_layers_sigmoid(number_of_neurons):\n",
        "  epochs = 20\n",
        "  learning_rate = 0.1\n",
        "  model = GenericFeedForwardNetwork(input_size, [number_of_neurons, number_of_neurons], output_size, 'sigmoid').to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "  los_fun = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  train_acc, validation_acc = training(epochs, optimizer, los_fun, model)\n",
        "  \n",
        "  plot_report(epochs, \"train accuracy by epochs\", train_acc, number_of_neurons)\n",
        "  plot_report(epochs, \"validation accuracy by epochs\", validation_acc, number_of_neurons)\n",
        "  \n",
        "\n",
        "  test_acc = calculate_accuracy(test_data, model,f\"{inspect.stack()[0].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJQ2lz1Afb7J"
      },
      "source": [
        "**Function 4**\n",
        "\n",
        "Two hidden layers ReLU\n",
        "\n",
        "The idea we went in the function is as follow, we are creating n models (n = number of different learning rates) and training them in a loop, each iteration of the loop we save the model and the data for train and validation accuracy.\n",
        "\n",
        "After finishing with training each model we find the validation accuracy for the given learning rate.\n",
        "\n",
        "Since all data are saved in different lists at the same iterations so the indices will match, best validation accuraccy on the model index == best model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwzfxqGqfi5u"
      },
      "source": [
        "def two_hidden_layers_relu(number_of_neurons):\n",
        "  \n",
        "  # Initial parameters\n",
        "  epochs = 20\n",
        "  learning_rate_list = np.arange(0.01, 1, 0.1)\n",
        "  los_fun = torch.nn.CrossEntropyLoss()\n",
        "  model_data_list = [] # A list to hold the data\n",
        "  val_acc_list = []\n",
        "\n",
        "  # Training models over different learning rates\n",
        "  for lr in learning_rate_list:\n",
        "    # Generating new parameters\n",
        "    model = GenericFeedForwardNetwork(input_size, [number_of_neurons, number_of_neurons], output_size, 'relu').to(device) # Model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = lr) # New optimizer for each learning rate\n",
        "    train_acc, val_acc = training(epochs, optimizer, los_fun, model) # Training on the new optimizer \n",
        "    model_data_list.append([model, train_acc, val_acc]) # Saving the model and the data for plots in \n",
        "\n",
        "    final_val_acc = calculate_accuracy(validation_data, model) \n",
        "    val_acc_list.append(final_val_acc) # Saving the validation accuracy on the trained model\n",
        "    \n",
        "  # Plotting the validation accuracy by learning rate\n",
        "  plot_report(learning_rate_list, \"validation accuracy by LR\", val_acc_list, number_of_neurons)\n",
        "\n",
        "  # Finding the max validation accuracy index \n",
        "  max_val_acc_index = val_acc_list.index(max(val_acc_list))\n",
        "  best_model_data = model_data_list[max_val_acc_index]\n",
        "  \n",
        "  # Plotting the training and validation accuracy by epochs\n",
        "  plot_report(epochs, \"train accuracy by epochs\", best_model_data[1], number_of_neurons)\n",
        "  plot_report(epochs, \"validation accuracy by epochs\", best_model_data[2], number_of_neurons)\n",
        "  \n",
        "  test_acc = calculate_accuracy(test_data, best_model_data[0], f\"{inspect.stack()[1].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4UYrE8dUMx_"
      },
      "source": [
        "**Fuction 5**\n",
        "\n",
        "Two hidden layers ReLU SGD learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IcwpmuJUKbS"
      },
      "source": [
        "def two_hidden_layers_relu_SGD_decreasing_lr(number_of_neurons):\n",
        "  learning_rate = 0.01\n",
        "  epochs = 20\n",
        "  step_size = epochs / 5\n",
        "  model = GenericFeedForwardNetwork(input_size,[number_of_neurons, number_of_neurons], output_size,'relu').to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)\n",
        "  \n",
        "  train_acc, validation_acc = training(epochs, optimizer, loss_fun, model,scheduler)\n",
        "\n",
        "  plot_by_epochs(epochs, 'Train', train_acc, number_of_neurons)\n",
        "  plot_by_epochs(epochs, 'Validaiton', validation_acc, number_of_neurons)\n",
        "  test_acc = calculate_accuracy(test_data, model, f\"{inspect.stack()[0].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRny-z4ui4LF"
      },
      "source": [
        "**Function 6**\n",
        "\n",
        "Two hidden layers ReLU Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuGmzvEi_2Z"
      },
      "source": [
        "def two_hidden_layers_relu_adam(number_of_neurons):\n",
        "  epochs = 30\n",
        "  learning_rate = 0.001\n",
        "  model = GenericFeedForwardNetwork(input_size, [number_of_neurons, number_of_neurons], output_size, 'relu').to(device)\n",
        "\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "  train_acc, validation_acc = training(epochs, optimizer, loss_fun, model)\n",
        "\n",
        "  plot_train_validation_by_epochs(epochs, train_acc, validation_acc, number_of_neurons)\n",
        "  \n",
        "  test_acc = calculate_accuracy(test_data, model, f\"{inspect.stack()[0].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZa0TM7YUwHX"
      },
      "source": [
        "**Function 7**\n",
        "\n",
        "Four hidden layers adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6CCaHotUvgW"
      },
      "source": [
        "def four_hidden_layers_adam(number_of_neurons):\n",
        "  epochs = 250\n",
        "  learning_rate = 0.001\n",
        "  train_data_len = len(train_data.dataset)\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "  new_train_data, rest_data = torch.utils.data.random_split(train_data.dataset, [int(0.1*train_data_len), int(0.9*train_data_len)])\n",
        "  new_train_dataloader = torch.utils.data.DataLoader(new_train_data, batch_size=64)\n",
        "  model = GenericFeedForwardNetwork(input_size,\n",
        "                                    [number_of_neurons,number_of_neurons, number_of_neurons,number_of_neurons],\n",
        "                                    output_size, 'relu').to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "  train_acc, validation_acc, train_loss, validation_loss, stop_point, min_point = training(epochs, optimizer, loss_fun, model, train_DL= new_train_dataloader, calc_loss=True)\n",
        "\n",
        "  plot_report(epochs, \"training loss by epochs\", train_loss, number_of_neurons)\n",
        "  plot_report(epochs, \"validation loss by epochs\", validation_loss, number_of_neurons)\n",
        "  plot_report(epochs, \"training accuracy by epochs\", train_acc, number_of_neurons)\n",
        "  plot_report(epochs, 'validation accuracy by epochs', validation_acc, number_of_neurons)\n",
        "\n",
        "  test_acc = calculate_accuracy(test_data, model, f\"{inspect.stack()[0].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxTqpoP7wNYT"
      },
      "source": [
        "**Function 8**\n",
        "\n",
        "Four hidden layers adam weight decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twapobkYwUOC"
      },
      "source": [
        "def four_hidden_layers_adam_weight_decay(number_of_neurons):\n",
        "  epochs = 250\n",
        "  learning_rate = 0.001\n",
        "  train_data_len = len(train_data.dataset)\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "  new_train_data, rest_data = torch.utils.data.random_split(train_data.dataset, [int(0.1*train_data_len), int(0.9*train_data_len)])\n",
        "  new_train_dataloader = torch.utils.data.DataLoader(new_train_data, batch_size=64)\n",
        "  model = GenericFeedForwardNetwork(input_size,\n",
        "                                    [number_of_neurons,number_of_neurons, number_of_neurons,number_of_neurons],\n",
        "                                    output_size, 'relu').to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay= 0.001)\n",
        "  train_acc, validation_acc, train_loss, validation_loss, stop_point, min_point = training(epochs, optimizer, loss_fun, model, train_DL= new_train_dataloader, calc_loss=True)\n",
        "\n",
        "  plot_report(epochs, \"training loss by epochs\", train_loss, number_of_neurons)\n",
        "  plot_report(epochs, \"validation loss by epochs\", validation_loss, number_of_neurons)\n",
        "  plot_report(epochs, \"training accuracy by epochs\", train_acc, number_of_neurons)\n",
        "  plot_report(epochs, 'validation accuracy by epochs', validation_acc, number_of_neurons)\n",
        "\n",
        "  test_acc = calculate_accuracy(test_data, model, f\"{inspect.stack()[0].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ4nxN3gLOBx"
      },
      "source": [
        "**Function 9**\n",
        "\n",
        "Four hidden layers adam early stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOCSOagaLVrk"
      },
      "source": [
        "def four_hidden_layers_adam_early_stopping(number_of_neurons):\n",
        "  epochs = 250\n",
        "  learning_rate = 0.001\n",
        "  train_data_len = len(train_data.dataset)\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "  new_train_data, rest_data = torch.utils.data.random_split(train_data.dataset, [int(0.1*train_data_len), int(0.9*train_data_len)])\n",
        "  new_train_dataloader = torch.utils.data.DataLoader(new_train_data, batch_size=64)\n",
        "  model = GenericFeedForwardNetwork(input_size,\n",
        "                                    [number_of_neurons,number_of_neurons, number_of_neurons,number_of_neurons],\n",
        "                                    output_size, 'relu').to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay= 0.001)\n",
        "  train_acc, validation_acc, train_loss, validation_loss, stop_point, min_point = training(epochs, optimizer, loss_fun, model, train_DL= new_train_dataloader, calc_loss=True, patience=20)\n",
        "  \n",
        "  plot_report(stop_point, \"training loss by epochs\", train_loss, number_of_neurons)\n",
        "  plot_report(stop_point, \"validation loss by epochs\", validation_loss, number_of_neurons)\n",
        "  plot_report(stop_point, \"training accuracy by epochs\", train_acc, number_of_neurons)\n",
        "  plot_report(stop_point, 'validation accuracy by epochs', validation_acc, number_of_neurons)\n",
        "  \n",
        "  model.load_state_dict(torch.load(\"./data/best_model.pth\"))\n",
        "  model.eval()\n",
        "  test_acc = calculate_accuracy(test_data, model , f\"{inspect.stack()[0].function}_{number_of_neurons}\")\n",
        "\n",
        "  print(f\"Test data accuracy is {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPajZLPG88-0"
      },
      "source": [
        "# Test Block\n",
        "\n",
        "number_of_neurons = 4\n",
        "one_hidden_layer_no_activation(number_of_neurons)\n",
        "two_hidden_layers_sigmoid(number_of_neurons)\n",
        "two_hidden_layers_relu(number_of_neurons)\n",
        "two_hidden_layers_relu_SGD_decreasing_lr(number_of_neurons)\n",
        "two_hidden_layers_relu_adam(number_of_neurons)\n",
        "\n",
        "number_of_neurons = 32\n",
        "one_hidden_layer_no_activation(number_of_neurons)\n",
        "two_hidden_layers_sigmoid(number_of_neurons)\n",
        "two_hidden_layers_relu(number_of_neurons)\n",
        "two_hidden_layers_relu_SGD_decreasing_lr(number_of_neurons)\n",
        "two_hidden_layers_relu_adam(number_of_neurons)\n",
        "four_hidden_layers_adam(number_of_neurons)\n",
        "four_hidden_layers_adam_weight_decay(number_of_neurons)\n",
        "four_hidden_layers_adam_early_stopping(number_of_neurons)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}